{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yr1bLDKvguqd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the IMDB dataset\n",
        "from tensorflow.keras.datasets import imdb\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)"
      ],
      "metadata": {
        "id": "iZwWDxwEg493"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert integer-encoded word sequences to text sequences\n",
        "word_index = imdb.get_word_index()\n",
        "index_to_word = {i: word for word, i in word_index.items()}\n",
        "x_train_text = [' '.join(index_to_word.get(i, '') for i in review) for review in x_train]\n",
        "x_test_text = [' '.join(index_to_word.get(i, '') for i in review) for review in x_test]\n",
        "\n",
        "# Create bag-of-words features\n",
        "vectorizer = CountVectorizer(max_features=10000)\n",
        "x_train_bow = vectorizer.fit_transform(x_train_text)\n",
        "x_test_bow = vectorizer.transform(x_test_text)\n"
      ],
      "metadata": {
        "id": "5esAtvDBg7-2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and validation sets\n",
        "x_train_bow, x_val_bow, y_train, y_val = train_test_split(x_train_bow, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=2000, random_state=42)\n",
        "model.fit(x_train_bow, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = model.predict(x_val_bow)\n",
        "\n",
        "# Calculate accuracy on the validation set\n",
        "accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(\"Validation Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZZxHy0ehC2O",
        "outputId": "2945e146-0573-42da-fc2d-1469a7566b90"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the test set\n",
        "y_test_pred = model.predict(x_test_bow)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvawBpgshqei",
        "outputId": "77aed1b4-7d95-4481-991f-5bd40364c115"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KdbuYY1ph8Xt"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}