{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f3a9b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb6a177",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec401363",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50328642",
   "metadata": {},
   "source": [
    "# Analyze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "537bd23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff8c49e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3170745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate target from features\n",
    "y_train = train['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c43b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Distributoin\n",
    "skewness = y_train.skew()\n",
    "kurtosis = y_train.kurt()\n",
    "\n",
    "print('Data Distribution of SalePrice(Target)')\n",
    "print(f'skewness: {skewness}')\n",
    "print(f'kurtosis: {kurtosis}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7334e9",
   "metadata": {},
   "source": [
    "First let's see how we measure the skewness and kurtosis:\n",
    "\n",
    "<h3>Skewness</h3>\n",
    "<ul>\n",
    "    <li><b>Positive: </b>right skewed</li>\n",
    "    <li><b>Negative: </b>left skewed</li>\n",
    "    <li><b>Near Zero: </b>approximately symmetrical</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Kurtosis</h3>\n",
    "<ul>\n",
    "    <li><b>Positive (Leptokurtic): </b>positive excess & peak at center (may have outlier)</li>\n",
    "    <li><b>Negative (Platykurtic): </b>negative excess (may have fewer outlier)</li>\n",
    "    <li><b>Near Zero (Mesokurtic): </b>normal distribution</li>\n",
    "</ul>\n",
    "\n",
    "    \n",
    "\n",
    "We can see that our data is `skewness: 1.88 -> right-skewed` and `kurtosis: 6.53 -> higher-kurtosis` (distributed highly towards right and with heavy tails at centers with outliers). To see this let's see our target value in histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bdf49a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of our target [SalePrice]\n",
    "y_train.hist(bins=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6f37d9",
   "metadata": {},
   "source": [
    "From the histogram above we can see that the data distribution is not uniform at all and have outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ccb91ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlaton matrix\n",
    "corr_matrix = train.corr()\n",
    "\n",
    "plt.figure(figsize=(33, 19))\n",
    "sns.set(font_scale=1.45)\n",
    "sns.heatmap(corr_matrix, square=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e4fe354",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = corr_matrix[\"SalePrice\"].sort_values(ascending=False)\n",
    "features = correlations.index[0:10]\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8daf239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train[features], height = 2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0109f20e",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae363cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_null = pd.isnull(train).sum()\n",
    "testing_null = pd.isnull(test).sum()\n",
    "\n",
    "null = pd.concat([training_null, testing_null], axis=1, keys=[\"Training\", \"Testing\"])\n",
    "null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06af28a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def miss_values_info(df):\n",
    "    # Assuming your DataFrame is named 'train'\n",
    "    # Calculate the percentage of missing values for each column\n",
    "    missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "    # Create a DataFrame to store the missing value information\n",
    "    missing_info = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Missing Values': df.isnull().sum(),\n",
    "        'Percentage': missing_percentage\n",
    "    })\n",
    "\n",
    "    # Filter the DataFrame to include only columns with missing values\n",
    "    missing_info = missing_info[missing_info['Missing Values'] > 0]\n",
    "    \n",
    "    # Sort the DataFrame by the percentage of missing values in descending order\n",
    "    missing_info = missing_info.sort_values(by='Percentage', ascending=False)\n",
    "\n",
    "    # Display the columns with the most missing values\n",
    "    return missing_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e9e14a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_missing = miss_values_info(train)\n",
    "print('train missing info: \\n', train_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fff0a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_missing = miss_values_info(test)\n",
    "print('test missing info: \\n', test_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99cb6ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on the description data file provided, all the variables who have meaningfull Nan\n",
    "\n",
    "null_with_meaning = [\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\", \"MasVnrType\"]\n",
    "\n",
    "#Replacing every Nan value with \"None\"\n",
    "\n",
    "for i in null_with_meaning:\n",
    "    train[i].fillna(\"None\", inplace=True)\n",
    "    test[i].fillna(\"None\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bf67be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_missing = miss_values_info(train)\n",
    "print('train missing info: \\n', train_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef71ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_missing = miss_values_info(test)\n",
    "print('test missing info: \\n', test_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e409e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def impute_missing_values(df):\n",
    "    # List of float type columns with missing values\n",
    "    float_columns = df.select_dtypes(include=['float64']).columns.tolist()\n",
    "\n",
    "    # List of object type columns with missing values\n",
    "    object_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Impute missing values for float type columns with the median\n",
    "    float_imputer = SimpleImputer(strategy='mean')\n",
    "    df[float_columns] = float_imputer.fit_transform(df[float_columns])\n",
    "\n",
    "    # Impute missing values for object type columns with the most frequent category ('mode')\n",
    "    object_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    df[object_columns] = object_imputer.fit_transform(df[object_columns])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e44288e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = impute_missing_values(train)\n",
    "test = impute_missing_values(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd131fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_missing = miss_values_info(train)\n",
    "print('train missing info: \\n', train_missing)\n",
    "print('\\n')\n",
    "test_missing = miss_values_info(test)\n",
    "print('test missing info: \\n', test_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1610603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = train.drop(columns=['SalePrice'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e627d7ce",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa9bb1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['LogPrice'] = np.log(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57654d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# First subplot: Histogram of y_train\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, first subplot\n",
    "plt.hist(y_train, bins=40, color='blue', alpha=0.7)\n",
    "plt.title(\"Histogram of SalePrice\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Second subplot: Histogram of train['LogPrice']\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, second subplot\n",
    "plt.hist(train['LogPrice'], bins=40, color='green', alpha=0.7)\n",
    "plt.title(\"Histogram of log(SalePrice)\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f6f96ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Distributoin\n",
    "skewness = y_train.skew()\n",
    "kurtosis = y_train.kurt()\n",
    "\n",
    "print('Data Distribution of SalePrice(Target)')\n",
    "print(f'skewness: {skewness}')\n",
    "print(f'kurtosis: {kurtosis}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "feaa6fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db8856cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def encode_categorical_columns(df):\n",
    "    # Identify categorical columns\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Initialize a LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    # List of object type columns with missing values\n",
    "    object_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Encode each object column\n",
    "    for column in object_columns:\n",
    "        df[column] = label_encoder.fit_transform(df[column])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ebc5e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train = encode_categorical_columns(train)\n",
    "encoded_test = encode_categorical_columns(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74479ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log = encoded_train['LogPrice']\n",
    "encoded_train.drop(columns=['LogPrice', 'SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99d085ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform on the training data\n",
    "train_scaled = scaler.fit_transform(encoded_train)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "test_scaled = scaler.transform(encoded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ae8bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_scaled, y_train_log, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6542e3f7",
   "metadata": {},
   "source": [
    "# Selecting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9874f196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "001c36f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.regression import *\n",
    "exp_1 = setup(data = train_scaled,  target = y_train_log)\n",
    "best_model = compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "891d064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9252b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pred = np.exp(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f28343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for submission\n",
    "submission_df = pd.DataFrame({'Id': test['Id'], 'SalePrice': y_pred})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission_df.to_csv('sample_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (nn)",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
