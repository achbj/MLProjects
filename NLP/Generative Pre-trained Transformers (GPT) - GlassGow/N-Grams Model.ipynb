{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78dd9300",
   "metadata": {},
   "source": [
    "# Lab 1: Building own n-gram language model\n",
    "\n",
    "For this we will start with already tokenized form of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c07536d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens of the sentences\n",
    "sample1 = ['It', 'shows', ',', 'my', 'dear', 'Watson', ',', 'that',\n",
    "           'we', 'are', 'dealing', 'with', 'an', 'exceptionally',\n",
    "           'astute', 'and', 'dangerous', 'man', '.']\n",
    "sample2 = ['How', 'would', 'Lausanne', 'do', ',', 'my', 'dear',\n",
    "           'Watson', '?']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cabecd4",
   "metadata": {},
   "source": [
    "Our first task is to write a function that splits the `tokens` sequence\n",
    "into its `n`-grams.\n",
    "\n",
    "For instance, when `tokens=sample1` and `n=3`, your function should\n",
    "return:\n",
    "\n",
    "```python\n",
    "[('It', 'shows', ','),\n",
    " ('shows', ',', 'my'),\n",
    " (',', 'my', 'dear'),\n",
    " ...,\n",
    " ('dangerous', 'man', '.')]\n",
    "```\n",
    "\n",
    "Note: You should return a python [`list`](https://docs.python.org/3/tutorial/datastructures.html#more-on-lists) containing [`tuple`](https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences)s. `tuple`s are immutable sequences, which will be useful later on when you build your language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "191b8353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'shows', ','),\n",
       " ('shows', ',', 'my'),\n",
       " (',', 'my', 'dear'),\n",
       " ('my', 'dear', 'Watson'),\n",
       " ('dear', 'Watson', ','),\n",
       " ('Watson', ',', 'that'),\n",
       " (',', 'that', 'we'),\n",
       " ('that', 'we', 'are'),\n",
       " ('we', 'are', 'dealing'),\n",
       " ('are', 'dealing', 'with'),\n",
       " ('dealing', 'with', 'an'),\n",
       " ('with', 'an', 'exceptionally'),\n",
       " ('an', 'exceptionally', 'astute'),\n",
       " ('exceptionally', 'astute', 'and'),\n",
       " ('astute', 'and', 'dangerous'),\n",
       " ('and', 'dangerous', 'man'),\n",
       " ('dangerous', 'man', '.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def build_ngrams(tokens: List[str], n: int) -> List[Tuple[str]]:\n",
    "    \"\"\"\n",
    "    This function takes a list of tokens and an integer n,\n",
    "    and returns a list of n-grams (as tuples) from the token list.\n",
    "    \"\"\"\n",
    "    ngrams = []\n",
    "    \n",
    "    for i in range(len(tokens) - n+1):\n",
    "        ngram = tuple(tokens[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "# Example\n",
    "build_ngrams(sample1, n=3)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dfb482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests:\n",
    "assert len(build_ngrams(sample1, n=3)) == 17\n",
    "assert build_ngrams(sample1, n=3)[0] == ('It', 'shows', ',')\n",
    "assert build_ngrams(sample1, n=3)[10] == ('dealing', 'with', 'an')\n",
    "assert len(build_ngrams(sample1, n=2)) == 18\n",
    "assert build_ngrams(sample1, n=2)[0] == ('It', 'shows')\n",
    "assert build_ngrams(sample1, n=2)[10] == ('dealing', 'with')\n",
    "assert len(build_ngrams(sample2, n=2)) == 8\n",
    "assert build_ngrams(sample2, n=2)[0] == ('How', 'would')\n",
    "assert build_ngrams(sample2, n=2)[7] == ('Watson', '?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7afd73",
   "metadata": {},
   "source": [
    "With the current function, there's no way to know whether an n-gram is at the beginning, middle, or end of the sequence. To overcome this problem, n-gram language models often include special \"beginning-of-string\" (BOS) and \"end-of-string\" (EOS) control tokens.\n",
    "\n",
    "Write a new version of our `build_ngrams` function that includes these control tokens. For instance, when `tokens=sample1` and `n=3`, your new function should return:\n",
    "\n",
    "```python\n",
    "[('<BOS>', '<BOS>', 'It'),\n",
    " ('<BOS>', 'It', 'shows'),\n",
    " ('It', 'shows', ','),\n",
    " ('shows', ',', 'my'),\n",
    " (',', 'my', 'dear'),\n",
    " ...,\n",
    " ('dangerous', 'man', '.'),\n",
    " ('man', '.', '<EOS>'),\n",
    " ('.', '<EOS>', '<EOS>')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a96ec34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<BOS>', '<BOS>', 'It'),\n",
       " ('<BOS>', 'It', 'shows'),\n",
       " ('It', 'shows', ','),\n",
       " ('shows', ',', 'my'),\n",
       " (',', 'my', 'dear'),\n",
       " ('my', 'dear', 'Watson'),\n",
       " ('dear', 'Watson', ','),\n",
       " ('Watson', ',', 'that'),\n",
       " (',', 'that', 'we'),\n",
       " ('that', 'we', 'are'),\n",
       " ('we', 'are', 'dealing'),\n",
       " ('are', 'dealing', 'with'),\n",
       " ('dealing', 'with', 'an'),\n",
       " ('with', 'an', 'exceptionally'),\n",
       " ('an', 'exceptionally', 'astute'),\n",
       " ('exceptionally', 'astute', 'and'),\n",
       " ('astute', 'and', 'dangerous'),\n",
       " ('and', 'dangerous', 'man'),\n",
       " ('dangerous', 'man', '.'),\n",
       " ('man', '.', '<EOS>'),\n",
       " ('.', '<EOS>', '<EOS>')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "\n",
    "def build_ngrams_ctrl(tokens: List[str], n: int) -> List[Tuple[str]]:\n",
    "    \"\"\"\n",
    "    Enhance the build_ngrams function to include beginning-of-sequence (BOS)\n",
    "    and end-of-sequence (EOS) control tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    modified_tokens = [BOS] * (n-1) + tokens + [EOS] * (n-1)\n",
    "    ngrams = []\n",
    "    \n",
    "    for i in range(len(modified_tokens) - n+1):\n",
    "        ngram = tuple(modified_tokens[i: i+n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "    \n",
    "# Example:\n",
    "build_ngrams_ctrl(sample1, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f134fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests:\n",
    "assert len(build_ngrams_ctrl(sample1, n=3)) == 21\n",
    "assert build_ngrams_ctrl(sample1, n=3)[0] == ('<BOS>', '<BOS>', 'It')\n",
    "assert build_ngrams_ctrl(sample1, n=3)[10] == ('we', 'are', 'dealing')\n",
    "assert len(build_ngrams_ctrl(sample1, n=2)) == 20\n",
    "assert build_ngrams_ctrl(sample1, n=2)[0] == ('<BOS>', 'It')\n",
    "assert build_ngrams_ctrl(sample1, n=2)[10] == ('are', 'dealing')\n",
    "assert len(build_ngrams_ctrl(sample2, n=2)) == 10\n",
    "assert build_ngrams_ctrl(sample2, n=2)[0] == ('<BOS>', 'How')\n",
    "assert build_ngrams_ctrl(sample2, n=2)[9] == ('?', '<EOS>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae9d345",
   "metadata": {},
   "source": [
    "Now that you can build n-grams, we have almost everything we need to build an n-gram language model.\n",
    "\n",
    "To compute Maximum Likelihood Estimations, you first need to count the number of times each word follows an n-gram of size `n-1`. You can build this structure as a Python [`dict`](https://docs.python.org/3/tutorial/datastructures.html#dictionaries) that maps the n-grams of size `n-1` to another `dict` that maps the following words to their respective counts.\n",
    "\n",
    "For instance, when `texts=[sample1, sample2]` and `n=3`, your function should return:\n",
    "\n",
    "```python\n",
    "{\n",
    "    ('<BOS>', '<BOS>'): {'It': 1, 'How': 1},\n",
    "    ('<BOS>', 'It'): {'shows': 1},\n",
    "    ('<BOS>', 'How'): {'would': 1},\n",
    "    ...\n",
    "    ('my', 'dear'): {'Watson': 2},\n",
    "    ('dear', 'Watson'): {',': 1, '?': 1},\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4086f1f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<BOS>', '<BOS>'): {'It': 1, 'How': 1},\n",
       " ('<BOS>', 'It'): {'shows': 1},\n",
       " ('It', 'shows'): {',': 1},\n",
       " ('shows', ','): {'my': 1},\n",
       " (',', 'my'): {'dear': 2},\n",
       " ('my', 'dear'): {'Watson': 2},\n",
       " ('dear', 'Watson'): {',': 1, '?': 1},\n",
       " ('Watson', ','): {'that': 1},\n",
       " (',', 'that'): {'we': 1},\n",
       " ('that', 'we'): {'are': 1},\n",
       " ('we', 'are'): {'dealing': 1},\n",
       " ('are', 'dealing'): {'with': 1},\n",
       " ('dealing', 'with'): {'an': 1},\n",
       " ('with', 'an'): {'exceptionally': 1},\n",
       " ('an', 'exceptionally'): {'astute': 1},\n",
       " ('exceptionally', 'astute'): {'and': 1},\n",
       " ('astute', 'and'): {'dangerous': 1},\n",
       " ('and', 'dangerous'): {'man': 1},\n",
       " ('dangerous', 'man'): {'.': 1},\n",
       " ('man', '.'): {'<EOS>': 1},\n",
       " ('.', '<EOS>'): {'<EOS>': 1},\n",
       " ('<BOS>', 'How'): {'would': 1},\n",
       " ('How', 'would'): {'Lausanne': 1},\n",
       " ('would', 'Lausanne'): {'do': 1},\n",
       " ('Lausanne', 'do'): {',': 1},\n",
       " ('do', ','): {'my': 1},\n",
       " ('Watson', '?'): {'<EOS>': 1},\n",
       " ('?', '<EOS>'): {'<EOS>': 1}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "def count_ngrams(texts: List[List[str]], n: int) -> Dict[Tuple[str, ...], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    counts the occurrences of each word following an n-gram\n",
    "    of size n-1 in the given text\n",
    "    \"\"\"\n",
    "    ngram_counts = {}\n",
    "    \n",
    "    for tokens in texts:\n",
    "        ngrams = build_ngrams_ctrl(tokens, n)\n",
    "        for ngram in ngrams:\n",
    "            prefix = ngram[:-1]\n",
    "            following_word = ngram[-1]\n",
    "            \n",
    "            if prefix not in ngram_counts:\n",
    "                ngram_counts[prefix] = {}\n",
    "            if following_word not in ngram_counts[prefix]:\n",
    "                ngram_counts[prefix][following_word] = 1\n",
    "            else:\n",
    "                ngram_counts[prefix][following_word] +=1\n",
    "    return ngram_counts\n",
    "\n",
    "# Example:\n",
    "count_ngrams([sample1, sample2], n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76af8f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests:\n",
    "assert len(count_ngrams([sample1, sample2], n=3)) == 28\n",
    "assert len(count_ngrams([sample1, sample2], n=3)['<BOS>', '<BOS>']) == 2\n",
    "assert count_ngrams([sample1, sample2], n=3)['<BOS>', '<BOS>']['It'] == 1\n",
    "assert count_ngrams([sample1, sample2], n=3)['<BOS>', '<BOS>']['How'] == 1\n",
    "assert count_ngrams([sample1, sample2], n=3)['my', 'dear']['Watson'] == 2\n",
    "assert len(count_ngrams([sample1, sample2], n=2)) == 24\n",
    "assert len(count_ngrams([sample1, sample2], n=2)['<BOS>',]) == 2\n",
    "assert count_ngrams([sample1, sample2], n=2)['<BOS>',]['It'] == 1\n",
    "assert count_ngrams([sample1, sample2], n=2)['<BOS>',]['How'] == 1\n",
    "assert count_ngrams([sample1, sample2], n=2)['dear',]['Watson'] == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477a0f2",
   "metadata": {},
   "source": [
    "We're almost there! The last step is to convert the counts into probability estimates.\n",
    "\n",
    "When `texts=[sample1, sample2]` and `n=3`, your function should return:\n",
    "\n",
    "```python\n",
    "{\n",
    "    ('<BOS>', '<BOS>'): {'It': 0.5, 'How': 0.5},\n",
    "    ('<BOS>', 'It'): {'shows': 1.0},\n",
    "    ('<BOS>', 'How'): {'would': 1.0},\n",
    "    ...\n",
    "    ('my', 'dear'): {'Watson': 1.0},\n",
    "    ('dear', 'Watson'): {',': 0.5, '?': 0.5},\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09faa179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<BOS>', '<BOS>'): {'It': 0.5, 'How': 0.5},\n",
       " ('<BOS>', 'It'): {'shows': 1.0},\n",
       " ('It', 'shows'): {',': 1.0},\n",
       " ('shows', ','): {'my': 1.0},\n",
       " (',', 'my'): {'dear': 1.0},\n",
       " ('my', 'dear'): {'Watson': 1.0},\n",
       " ('dear', 'Watson'): {',': 0.5, '?': 0.5},\n",
       " ('Watson', ','): {'that': 1.0},\n",
       " (',', 'that'): {'we': 1.0},\n",
       " ('that', 'we'): {'are': 1.0},\n",
       " ('we', 'are'): {'dealing': 1.0},\n",
       " ('are', 'dealing'): {'with': 1.0},\n",
       " ('dealing', 'with'): {'an': 1.0},\n",
       " ('with', 'an'): {'exceptionally': 1.0},\n",
       " ('an', 'exceptionally'): {'astute': 1.0},\n",
       " ('exceptionally', 'astute'): {'and': 1.0},\n",
       " ('astute', 'and'): {'dangerous': 1.0},\n",
       " ('and', 'dangerous'): {'man': 1.0},\n",
       " ('dangerous', 'man'): {'.': 1.0},\n",
       " ('man', '.'): {'<EOS>': 1.0},\n",
       " ('.', '<EOS>'): {'<EOS>': 1.0},\n",
       " ('<BOS>', 'How'): {'would': 1.0},\n",
       " ('How', 'would'): {'Lausanne': 1.0},\n",
       " ('would', 'Lausanne'): {'do': 1.0},\n",
       " ('Lausanne', 'do'): {',': 1.0},\n",
       " ('do', ','): {'my': 1.0},\n",
       " ('Watson', '?'): {'<EOS>': 1.0},\n",
       " ('?', '<EOS>'): {'<EOS>': 1.0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "def build_ngram_model(texts: List[List[str]], n: int) -> Dict[Tuple[str, ...], Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Convert n-gram counts to probability estimates.\n",
    "    \"\"\"\n",
    "    ngram_probabilities = {}\n",
    "    ngram_counts = count_ngrams(texts, n)\n",
    "    \n",
    "    for prefix, following_words in ngram_counts.items():\n",
    "        total_count = sum(following_words.values())\n",
    "        ngram_probabilities[prefix] = {word: count / total_count for word, count in following_words.items()}\n",
    "    \n",
    "    return ngram_probabilities\n",
    "\n",
    "# Example:\n",
    "build_ngram_model([sample1, sample2], n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c595fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests:\n",
    "assert build_ngram_model([sample1, sample2], n=3)['<BOS>', '<BOS>']['It'] == 0.5\n",
    "assert build_ngram_model([sample1, sample2], n=3)['<BOS>', '<BOS>']['How'] == 0.5\n",
    "assert build_ngram_model([sample1, sample2], n=3)['my', 'dear']['Watson'] == 1.0\n",
    "assert build_ngram_model([sample1, sample2], n=2)['<BOS>',]['It'] == 0.5\n",
    "assert build_ngram_model([sample1, sample2], n=2)['<BOS>',]['How'] == 0.5\n",
    "assert build_ngram_model([sample1, sample2], n=2)['dear',]['Watson'] == 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d4d26a",
   "metadata": {},
   "source": [
    "A language model built from only a few sentences is not very informative. Let's scale up and see what your language model looks like when we train on the complete works of Sir Arthur Conon Doyle!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5714e383",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = []\n",
    "with open('arthur-conan-doyle.tok.train.txt', 'rt') as fin:\n",
    "    for line in fin:\n",
    "        full_text.append(list(line.split()))\n",
    "        \n",
    "model = build_ngram_model(full_text, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce2adad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> <BOS>\n",
      "\t\"\t0.8073\n",
      "\tThe\t0.0304\n",
      "\tHolmes\t0.0230\n",
      "\tI\t0.0167\n",
      "\tIt\t0.0141\n",
      "\t[206 more...]\n",
      "<BOS> It\n",
      "\twas\t0.8235\n",
      "\tis\t0.0515\n",
      "\tmay\t0.0221\n",
      "\thad\t0.0147\n",
      "\tseemed\t0.0074\n",
      "\t[11 more...]\n",
      "It was\n",
      "\ta\t0.1888\n",
      "\tthe\t0.0686\n",
      "\tnot\t0.0562\n",
      "\tonly\t0.0312\n",
      "\tan\t0.0250\n",
      "\t[184 more...]\n",
      "my dear\n",
      "\tWatson\t0.5612\n",
      "\tfellow\t0.1429\n",
      "\tsir\t0.0918\n",
      "\tyoung\t0.0510\n",
      "\tVon\t0.0204\n",
      "\t[12 more...]\n"
     ]
    }
   ],
   "source": [
    "for prefix in [(BOS, BOS), (BOS, 'It'), ('It', 'was'), ('my', 'dear')]:\n",
    "    print(*prefix)\n",
    "    sorted_probs = sorted(model[prefix].items(), key=lambda x: -x[1])\n",
    "    for k, v in sorted_probs[:5]:\n",
    "        print(f'\\t{k}\\t{v:.4f}')\n",
    "    print(f'\\t[{len(sorted_probs)-5} more...]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1f59b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS> As I glanced at each of them which he handed it to the south of France . This time his\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_sentence(model, n=3, start_tokens=('<BOS>',), stop_token='<EOS>', max_length=20):\n",
    "    current_prefix = tuple(start_tokens) * (n-1)  # Adjust for n-gram size\n",
    "    sentence = list(current_prefix[:-1])  # Initialize sentence\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        next_words = model.get(current_prefix)\n",
    "        if not next_words:\n",
    "            break  # Stop if the prefix is not found\n",
    "        \n",
    "        # Choose the next word based on the probabilities\n",
    "        next_word = random.choices(list(next_words.keys()), weights=next_words.values())[0]\n",
    "        if next_word == stop_token:\n",
    "            break  # Stop if the end-of-sequence token is reached\n",
    "        \n",
    "        sentence.append(next_word)\n",
    "        # Update the prefix\n",
    "        current_prefix = (*current_prefix[1:], next_word)\n",
    "    \n",
    "    return ' '.join(sentence)\n",
    "\n",
    "# Example usage\n",
    "BOS = '<BOS>'\n",
    "EOS = '<EOS>'\n",
    "# Assuming `model` is your trained n-gram model\n",
    "sentence = generate_sentence(model, n=3, start_tokens=(BOS,), stop_token=EOS)\n",
    "print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99c6c28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My house is on the country . \" You are not a dozen of them in the passage with as little as if they were papers that he shall go mad if I do n't know when you sit on the day when this fellow . He kept the key in the opalescent London reek . Holmes looked thoroughly surprised at my watch . \" A sheath - knife . <EOS> <EOS>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_from_ngram(model, start_prefix, num_words=50):\n",
    "    \"\"\"\n",
    "    Generate words based on a given start prefix using the n-gram model.\n",
    "    \n",
    "    Parameters:\n",
    "        model (dict): The n-gram model as a nested dictionary.\n",
    "        start_prefix (tuple): The starting prefix as a tuple of words.\n",
    "        num_words (int): The maximum number of words to generate.\n",
    "        \n",
    "    Returns:\n",
    "        A list of generated words.\n",
    "    \"\"\"\n",
    "    generated_words = list(start_prefix)\n",
    "    current_prefix = start_prefix\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        if current_prefix not in model or not model[current_prefix]:\n",
    "            break  # Stop if no continuation is found or the model doesn't have the current prefix\n",
    "        \n",
    "        # Select the next word based on the distribution of following words\n",
    "        next_words = model[current_prefix]\n",
    "        choices, weights = zip(*next_words.items())\n",
    "        next_word = random.choices(choices, weights=weights, k=1)[0]\n",
    "        \n",
    "        # Update the prefix and generated words\n",
    "        generated_words.append(next_word)\n",
    "        current_prefix = (*current_prefix[1:], next_word)  # Update prefix by removing the first word and adding the new one\n",
    "    \n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "# Example usage\n",
    "BOS, EOS = '<BOS>', '<EOS>'\n",
    "# Assuming `model` is your n-gram model built from the text\n",
    "# start_prefix = (BOS, BOS)  # For generating new text from the beginning\n",
    "# Or for user input:\n",
    "user_input = ('My', 'house',)\n",
    "generated_text = generate_from_ngram(model, user_input, num_words=500)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df0b881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (nn)",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
